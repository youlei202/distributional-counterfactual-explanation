{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from models.mlp import BlackBoxModel\n",
    "from models.lr import LogisticRegression\n",
    "from models.svm import LinearSVM\n",
    "from models.rbf import RBFNet\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv('data/cardio/cardio.csv', sep=';')\n",
    "df = df_.drop(columns=['id'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = 'cardio'\n",
    "target = df[target_name].replace({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'age', \n",
    "    'gender', \n",
    "    'height', \n",
    "    'weight', \n",
    "    'ap_hi', \n",
    "    'ap_lo', \n",
    "    'cholesterol',\n",
    "    'gluc', \n",
    "    'smoke', \n",
    "    'alco', \n",
    "    'active'\n",
    "]\n",
    "\n",
    "df_X = df[features].copy()\n",
    "df_y = df[target_name].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "np.random.seed(seed)  # for reproducibility\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=seed)\n",
    "\n",
    "std = X_train.std()\n",
    "mean = X_train.mean()\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "# X_train, X_test, y_train, y_test = X_train.values, X_test.values, y_train.values, y_test.values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train.values)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).view(-1, 1)\n",
    "X_test_tensor = torch.FloatTensor(X_test.values)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).view(-1, 1)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "# model = LinearSVM(input_dim=X_train.shape[1])\n",
    "# model = LogisticRegression(input_dim=X_train.shape[1])\n",
    "model = BlackBoxModel(input_dim=X_train.shape[1])\n",
    "# model = RBFNet(input_dim=X_train.shape[1], hidden_dim=X_train.shape[1], output_dim=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_loss = criterion(test_outputs, y_test_tensor)\n",
    "\n",
    "    # Convert outputs to binary using 0.5 as threshold\n",
    "    y_pred_tensor = (test_outputs > 0.5).float()\n",
    "    correct_predictions = (y_pred_tensor == y_test_tensor).float().sum()\n",
    "    accuracy = correct_predictions / y_test_tensor.shape[0]\n",
    "\n",
    "accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 100\n",
    "delta=0.15\n",
    "alpha=0.1\n",
    "N=10\n",
    "explain_columns = [\n",
    "    'age', \n",
    "    'gender', \n",
    "    'height', \n",
    "    'weight', \n",
    "    'ap_hi', \n",
    "    'ap_lo', \n",
    "    'cholesterol',\n",
    "    'gluc', \n",
    "    'smoke', \n",
    "    'alco', \n",
    "    'active'\n",
    "]\n",
    "\n",
    "indice = (X_test.sample(sample_num)).index\n",
    "\n",
    "df_explain = X_test.loc[indice]\n",
    "\n",
    "# X = X_test.loc[indice].values\n",
    "y = model(torch.FloatTensor(df_explain.values))\n",
    "\n",
    "y_target = torch.round(torch.distributions.beta.Beta(0.2, 0.3).sample((sample_num,)))\n",
    "\n",
    "y_true = y_test.loc[indice]\n",
    "\n",
    "# Sort the arrays\n",
    "y_target_sorted = np.sort(y_target.numpy())\n",
    "y_true_sorted = np.sort(y_true)\n",
    "y_sorted = np.sort(y.squeeze().detach().numpy())\n",
    "\n",
    "# Generate quantiles\n",
    "quantiles = np.linspace(0, 1, sample_num)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(quantiles, y_target_sorted, label='y_target (Beta Dist.)', color='red')\n",
    "plt.plot(quantiles, y_true_sorted, label='y_true', color='black')\n",
    "plt.plot(quantiles, y_sorted, label='y_model', color='green')\n",
    "\n",
    "\n",
    "plt.title('Quantile Function')\n",
    "plt.xlabel('Quantiles')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explainers.dce import DistributionalCounterfactualExplainer\n",
    "\n",
    "explainer = DistributionalCounterfactualExplainer(\n",
    "    model=model, \n",
    "    df_X=df_explain, \n",
    "    explain_columns=explain_columns,\n",
    "    y_target=y_target, \n",
    "    lr=1e-1, \n",
    "    n_proj=10,\n",
    "    delta=delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(explainer.wd.distance(y, y_target, delta=delta)[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.wd.distance_interval(y, y_target, delta=delta, alpha=alpha, bootstrap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = explainer.X[:, explainer.explain_indices].clone() \n",
    "X_t = explainer.X_prime[:, explainer.explain_indices].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(explainer.swd.distance(X_s, X_t, delta)[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.swd.distance_interval(X_s, X_t, delta=delta, alpha=alpha, bootstrap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8*(1-0.04)**150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.optimize(U_1=0.1, U_2=0.25, l=0, r=1, max_iter=100, tau=1e1, kappa=0.02, bootstrap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "# import pickle \n",
    "# dump_data_path = './data/cardio/'\n",
    "# with open(os.path.join(dump_data_path, \"explainer_convergence.pkl\"), \"wb\") as file:\n",
    "#     pickle.dump(explainer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "# import pickle \n",
    "# dump_data_path = './data/cardio/'\n",
    "# with open(os.path.join(dump_data_path, \"explainer_convergence.pkl\"), 'rb') as file:\n",
    "#     explainer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_qu = \"\"\n",
    "s_u1 = \"\"\n",
    "\n",
    "points_qu = list(map(lambda x: x.item(), explainer.swd_upper_list))\n",
    "# points_list = list(map(lambda x: x.item(), explainer.swd_upper_list))[:iteration]\n",
    "# points_list = list(map(lambda x: x.item() if type(x)==torch.Tensor else x, explainer.eta_list))\n",
    "# points_list = explainer.interval_right_list\n",
    "\n",
    "for i, v in enumerate(points_qu):\n",
    "    s_qu += f\"({i},{v})\"\n",
    "    s_u1 += f\"({i},0.1)\"\n",
    "print(\"Qu, U1\")\n",
    "print(s_qu)\n",
    "print(s_u1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_qv = \"\"\n",
    "s_u2 = \"\"\n",
    "\n",
    "points_qv = list(map(lambda x: x.item(), explainer.wd_upper_list))\n",
    "\n",
    "for i, v in enumerate(points_qv):\n",
    "    s_qv += f\"({i},{v})\"\n",
    "    s_u2 += f\"({i},0.25)\"\n",
    "print(\"Qv, U2\")\n",
    "print(s_qv)\n",
    "print(s_u2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_eta = \"\"\n",
    "s_eta_r = \"\"\n",
    "s_eta_l = \"\"\n",
    "eta_points = list(map(lambda x: x.item() if type(x)==torch.Tensor else x, explainer.eta_list))\n",
    "eta_r_points = explainer.interval_left_list\n",
    "eta_l_points = explainer.interval_right_list\n",
    "\n",
    "for i, v in enumerate(eta_points):\n",
    "    s_eta += f\"({i},{v})\"\n",
    "\n",
    "for i, v in enumerate(eta_r_points):\n",
    "    s_eta_r += f\"({i},{v})\"\n",
    "\n",
    "for i, v in enumerate(eta_l_points):\n",
    "    s_eta_l += f\"({i},{v})\"\n",
    "\n",
    "print(\"eta, (l,r)\")\n",
    "print(s_eta)\n",
    "print(s_eta_r)\n",
    "print(s_eta_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_X = explainer.best_X\n",
    "\n",
    "factual_X = df[df_X.columns].loc[indice].copy()\n",
    "counterfactual_X = pd.DataFrame(\n",
    "    which_X.detach().numpy() * std[df_X.columns].values + mean[df_X.columns].values,\n",
    "    columns=df_X.columns,\n",
    ")\n",
    "\n",
    "dtype_dict = df.dtypes.apply(lambda x: x.name).to_dict()\n",
    "for k, v in dtype_dict.items():\n",
    "    if k in counterfactual_X.columns:\n",
    "        if v[:3] == \"int\":\n",
    "            counterfactual_X[k] = counterfactual_X[k].round().astype(v)\n",
    "        else:\n",
    "            counterfactual_X[k] = counterfactual_X[k].astype(v)\n",
    "\n",
    "factual_y = pd.DataFrame(\n",
    "    y.detach().numpy(), columns=[target_name], index=factual_X.index\n",
    ")\n",
    "counterfactual_y = pd.DataFrame(\n",
    "    explainer.y.detach().numpy(), columns=[target_name], index=factual_X.index\n",
    ")\n",
    "\n",
    "# Recover the type of counterfactual_X\n",
    "dtype_dict = df.dtypes.apply(lambda x: x.name).to_dict()\n",
    "for k, v in dtype_dict.items():\n",
    "    if k in counterfactual_X.columns:\n",
    "        if v[:3] == \"int\":\n",
    "            counterfactual_X[k] = counterfactual_X[k].round().astype(v)\n",
    "        else:\n",
    "            counterfactual_X[k] = counterfactual_X[k].astype(v)\n",
    "\n",
    "counterfactual_X.index = factual_X.index\n",
    "counterfactual_X[target_name] = counterfactual_y\n",
    "factual_X[target_name] = factual_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'factual_y': factual_y[target_name].values,\n",
    "    'counterfactual_y': counterfactual_y[target_name].values,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle \n",
    "\n",
    "dump_data_path = './data/cardio/'\n",
    "\n",
    "factual_X.to_csv(os.path.join(dump_data_path, \"factual.csv\"), index=False)\n",
    "counterfactual_X.to_csv(\n",
    "    os.path.join(dump_data_path, \"counterfactual.csv\"), index=False\n",
    ")\n",
    "with open(os.path.join(dump_data_path, \"explainer.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(explainer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fontsize = 25\n",
    "\n",
    "# Enable LaTeX text rendering in Matplotlib\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"text.latex.preamble\": r\"\\usepackage{times}\",  # Ensure you use the times package\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times\", \"Times New Roman\"],  # This should use Times font\n",
    "    \"font.size\": fontsize\n",
    "})\n",
    "\n",
    "matrix_nu = explainer.wd.nu.detach().numpy()\n",
    "\n",
    "mu_avg = torch.zeros_like(explainer.swd.mu_list[0])\n",
    "for mu in explainer.swd.mu_list:\n",
    "    mu_avg += mu\n",
    "\n",
    "total_sum = mu_avg.sum()\n",
    "\n",
    "matrix_mu = mu_avg / total_sum\n",
    "\n",
    "# Determine the global minimum and maximum values across both matrices\n",
    "vmin = min(matrix_mu.min(), matrix_nu.min())\n",
    "vmax = max(matrix_mu.max(), matrix_nu.max())\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 8), facecolor='white')  # Set figure background to white\n",
    "\n",
    "# Set the background of each axis to white\n",
    "for ax in axs:\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "# First subplot for matrix_mu with 'Blues' color map\n",
    "im_mu = axs[0].imshow(matrix_mu, cmap='Blues', vmin=vmin, vmax=vmax)\n",
    "axs[0].set_title(\"Heatmap of $\\mu$\")\n",
    "\n",
    "# Second subplot for matrix_nu with 'Blues' color map\n",
    "im_nu = axs[1].imshow(matrix_nu, cmap='Blues', vmin=vmin, vmax=vmax)\n",
    "axs[1].set_title(\"Heatmap of $\\\\nu$\")\n",
    "\n",
    "# Create a colorbar for the whole figure\n",
    "fig.colorbar(im_mu, ax=axs, orientation='vertical')\n",
    "\n",
    "plt.savefig('pictures/transportation_plan.eps', format='eps', bbox_inches='tight')\n",
    "\n",
    "\n",
    "# # Display the plots\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_num  = 20\n",
    "\n",
    "# Interleave rows\n",
    "combined = pd.concat([factual_X.head(row_num), counterfactual_X.head(row_num)]).sort_index(kind='merge')\n",
    "\n",
    "# Define formatters for specific columns\n",
    "formatters = {\n",
    "    \"cardio\": lambda x: f\"{x:.4f}\",\n",
    "    \"weight\": lambda x: f\"{x:.1f}\"\n",
    "}\n",
    "\n",
    "\n",
    "# Convert to LaTeX\n",
    "latex_code = combined.to_latex(index=False, formatters=formatters, \n",
    "                               caption=\"[\\\\textit{{German-Credit}}] Data points of factual and counterfactual distributions.\", label=\"tab:german-credit\")\n",
    "\n",
    "print(latex_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
