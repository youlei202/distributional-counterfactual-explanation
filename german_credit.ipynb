{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German Credit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leiyo\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15624\\3136874902.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBlackBoxModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'display.max_columns'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from models.mlp import BlackBoxModel\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv('data/german_credit/german_credit_data.csv')\n",
    "df = df_.copy()\n",
    "\n",
    "target_name = 'Risk'\n",
    "target = df[target_name].replace({'good': 0, 'bad': 1})\n",
    "\n",
    "df['Risk'] = target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_mappings = {}\n",
    "\n",
    "\n",
    "# Convert categorical columns to numerical representations using label encoding\n",
    "for column in df.columns:\n",
    "    if column is not target_name and df[column].dtype == 'object':\n",
    "        # Handle missing values by filling with a placeholder and then encoding\n",
    "        df[column] = df[column].fillna('Unknown')\n",
    "        df[column] = label_encoder.fit_transform(df[column])\n",
    "        label_mappings[column] = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
    "\n",
    "\n",
    "# For columns with NaN values that are numerical, we will impute them with the median of the column\n",
    "for column in df.columns:\n",
    "    if df[column].isna().any():\n",
    "        median_val = df[column].median()\n",
    "        df[column].fillna(median_val, inplace=True)\n",
    "\n",
    "# Display the first few rows of the transformed dataframe\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'Age', \n",
    "    'Sex', \n",
    "    'Job', \n",
    "    'Housing', \n",
    "    'Saving accounts', \n",
    "    'Checking account',\n",
    "    'Credit amount', \n",
    "    'Duration', \n",
    "    'Purpose', \n",
    "]\n",
    "\n",
    "df_X = df[features].copy()\n",
    "df_y = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "np.random.seed(seed)  # for reproducibility\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=seed)\n",
    "\n",
    "std = X_train.std()\n",
    "mean = X_train.mean()\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "# X_train, X_test, y_train, y_test = X_train.values, X_test.values, y_train.values, y_test.values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train.values)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).view(-1, 1)\n",
    "X_test_tensor = torch.FloatTensor(X_test.values)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).view(-1, 1)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BlackBoxModel(input_dim=X_train.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_loss = criterion(test_outputs, y_test_tensor)\n",
    "\n",
    "    # Convert outputs to binary using 0.5 as threshold\n",
    "    y_pred_tensor = (test_outputs > 0.5).float()\n",
    "    correct_predictions = (y_pred_tensor == y_test_tensor).float().sum()\n",
    "    accuracy = correct_predictions / y_test_tensor.shape[0]\n",
    "\n",
    "accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 100\n",
    "delta = 0.1\n",
    "alpha = 0.05\n",
    "N=10\n",
    "explain_columns = [\n",
    "    'Age', \n",
    "    'Sex', \n",
    "    'Job', \n",
    "    'Housing', \n",
    "    'Saving accounts', \n",
    "    'Checking account',\n",
    "    'Credit amount', \n",
    "    'Duration', \n",
    "    'Purpose', \n",
    "]\n",
    "\n",
    "indice = (X_test.sample(sample_num)).index\n",
    "\n",
    "df_explain = X_test.loc[indice]\n",
    "\n",
    "# X = X_test.loc[indice].values\n",
    "y = model(torch.FloatTensor(df_explain.values))\n",
    "\n",
    "y_target = torch.distributions.beta.Beta(0.1, 0.9).sample((sample_num,))\n",
    "\n",
    "y_true = y_test.loc[indice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explainers.dce import DistributionalCounterfactualExplainer\n",
    "\n",
    "explainer = DistributionalCounterfactualExplainer(\n",
    "    model=model, \n",
    "    df_X=df_explain, \n",
    "    explain_columns=explain_columns,\n",
    "    y_target=y_target, \n",
    "    lr=1e-1, \n",
    "    n_proj=N,\n",
    "    delta=delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(explainer.wd.distance(y, y_target, delta=delta)[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.wd.distance_interval(y, y_target, delta=delta, alpha=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.optimize(U_1=0.5, U_2=0.3, l=0.2, r=1, max_iter=20, tau=1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.best_X = explainer.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = explainer.best_X[:, explainer.explain_indices].clone()\n",
    "X_t = explainer.X_prime.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(explainer.swd.distance(X_s, X_t, delta)[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.swd.distance_interval(X_s, X_t, delta=delta, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factual_X = df[df_X.columns].loc[indice].copy()\n",
    "counterfactual_X = pd.DataFrame(explainer.best_X.detach().numpy() * std[df_X.columns].values + mean[df_X.columns].values, columns=df_X.columns)\n",
    "\n",
    "dtype_dict = df.dtypes.apply(lambda x: x.name).to_dict()\n",
    "for k, v in dtype_dict.items():\n",
    "    if k in counterfactual_X.columns:\n",
    "        if v[:3] == 'int':\n",
    "            counterfactual_X[k] = counterfactual_X[k].round().astype(v)\n",
    "        else:\n",
    "            counterfactual_X[k] = counterfactual_X[k].astype(v)\n",
    "\n",
    "factual_y = pd.DataFrame(y.detach().numpy(),columns=[target_name], index=factual_X.index)\n",
    "counterfactual_y = pd.DataFrame(explainer.y.detach().numpy(),columns=[target_name], index=factual_X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'factual_y': factual_y[target_name].values,\n",
    "    'counterfactual_y': counterfactual_y[target_name].values,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_X.index = factual_X.index\n",
    "counterfactual_X[target_name] = counterfactual_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factual_X[target_name] = factual_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factual_X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_column = 'lead_time'\n",
    "pd.DataFrame({\n",
    "    'factual': factual_X[check_column].values, \n",
    "    'counterfactual': counterfactual_X[check_column].values\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factual_X[check_column].mean(), counterfactual_X[check_column].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample matrix for demonstration\n",
    "matrix = explainer.wd.nu.numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(matrix, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title(\"Heatmap of the Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
