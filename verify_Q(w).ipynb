{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple neural network for b\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 10)  # Hidden layer with 10 neurons\n",
    "        self.fc2 = nn.Linear(10, 1)          # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Reinitializing the data\n",
    "n = 3\n",
    "K = 4\n",
    "\n",
    "# Random vectors and matrices for testing\n",
    "X = np.array([[1, 2, 3], [2,3,4], [3,1,5]])\n",
    "X_prime = np.array([[4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "Theta = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [0.5, 0.5, 0.5]])\n",
    "M = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "lambda_val = 2.5\n",
    "nu_sample = np.array([[1.5, 2.5, 3.5, 4.5], \n",
    "                      [2.5, 3.5, 4.5, 5.5], \n",
    "                      [3.5, 4.5, 5.5, 6.5]])\n",
    "\n",
    "model = SimpleNN(X[0].shape[0])\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_derivative(Theta, M, x_i, X_prime, model, criterion, lambda_val, nu):\n",
    "    \"\"\"\n",
    "    Computes the derivative including the additional term using rigorous linear algebra operations.\n",
    "    The function b is approximated using a simple neural network.\n",
    "    \n",
    "    Parameters:\n",
    "    - Theta: The matrix of theta vectors.\n",
    "    - M: The matrix of mu coefficients.\n",
    "    - x_i: The vector x_i.\n",
    "    - X_prime: The matrix containing vectors x'_j as columns.\n",
    "    - lambda_val: Scalar constant lambda.\n",
    "    - nu: Matrix of coefficients nu.\n",
    "    \n",
    "    Returns:\n",
    "    - The computed derivative.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Calculate the difference matrix Delta\n",
    "    ones_vector = np.ones((X_prime.shape[0], 1))\n",
    "    broadcasted_xi = np.outer(Theta.dot(x_i), ones_vector.T)\n",
    "    Delta_rigorous = broadcasted_xi - Theta.dot(X_prime.T)\n",
    "\n",
    "    # Step 2: Element-wise multiplication by mu_{ij}^{(theta)}\n",
    "    WeightedDiff_rigorous = M * Delta_rigorous\n",
    "\n",
    "    # Step 3: Compute the final matrix representation\n",
    "    ResultMatrix_rigorous = np.dot(Theta.T, WeightedDiff_rigorous)\n",
    "\n",
    "    # Step 4: Sum over the columns to obtain the final result\n",
    "    FinalResult_rigorous = ResultMatrix_rigorous.dot(ones_vector).flatten()\n",
    "\n",
    "    # Additional term computation using PyTorch\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Convert x_i and X_prime to PyTorch tensors\n",
    "    x_i_tensor = torch.tensor(x_i, dtype=torch.float32, requires_grad=True)\n",
    "    X_prime_tensor = torch.tensor(X_prime, dtype=torch.float32)\n",
    "    \n",
    "    # Forward pass for b(x_i)\n",
    "    b_xi = model(x_i_tensor)\n",
    "\n",
    "    # Forward pass for b(x'_j)\n",
    "    b_x_prime = model(X_prime_tensor.T)\n",
    "    \n",
    "    # Compute the difference\n",
    "    diff_b = (b_xi - b_x_prime).T\n",
    "    \n",
    "    # Compute the loss and perform backpropagation\n",
    "    loss = criterion(b_xi, b_x_prime.mean())\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute the loss and perform backpropagation\n",
    "    loss.backward()\n",
    "    db_xi = x_i_tensor.grad.numpy()\n",
    "    \n",
    "    # Combine the results from PyTorch with the previous computations\n",
    "    summation_term = (nu * diff_b.detach().numpy().T).sum(axis=1)\n",
    "    additional_term = lambda_val * summation_term * db_xi\n",
    "    \n",
    "    # Combine the results\n",
    "    derivative = FinalResult_rigorous + additional_term\n",
    "    \n",
    "    return derivative\n",
    "\n",
    "def compute_gradient_Q(X, Theta, M, x_prime, model, criterion, lambda_val, nu):\n",
    "    \"\"\"\n",
    "    Computes the gradient of Q(x) using rigorous linear algebra operations.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: The vector for which the gradient is computed.\n",
    "    - Theta: The matrix of theta vectors.\n",
    "    - M: The matrix of mu coefficients.\n",
    "    - x_prime: The matrix containing vectors x'_j as columns.\n",
    "    \n",
    "    Returns:\n",
    "    - The gradient of Q(x).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty result matrix to store the derivatives for each x_i\n",
    "    gradient_matrix = np.zeros(X.shape)\n",
    "    \n",
    "    # Iterate over each element of x and compute the derivative\n",
    "    for i in range(X.shape[0]):\n",
    "        gradient_matrix[i] = compute_derivative(Theta, M, X[i], x_prime, model, criterion, lambda_val, nu)\n",
    "    \n",
    "    # # Sum over the rows to obtain the final result for each x_i\n",
    "    # gradient = gradient_matrix.sum(axis=0)\n",
    "    \n",
    "    return gradient_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-214.26692291, -309.36828849, -390.46088327],\n",
       "       [-176.75152936, -252.32785901, -318.77401467],\n",
       "       [-180.73966363, -259.68517924, -287.24941694]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the function\n",
    "compute_gradient_Q(X, Theta, M, X_prime, model, criterion, lambda_val, nu_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
